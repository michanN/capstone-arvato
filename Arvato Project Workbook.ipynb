{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import progressbar\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display more rows and columns\n",
    "def display_all(df, rows=100):\n",
    "    \"\"\"\n",
    "    Displays 1000 rows and columns\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to display\n",
    "    \"\"\"\n",
    "    with pd.option_context(\"display.max_rows\", rows, \"display.max_columns\", 1000):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data and use string for column 18 and 19\n",
    "#azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';', dtype={18:'str',19:'str'})\n",
    "#customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';', dtype={18:'str',19:'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "#azdias.to_pickle('data/azdias.pkl')\n",
    "#customers.to_pickle('data/customers.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle file\n",
    "azdias = pd.read_pickle('data/azdias.pkl')\n",
    "customers = pd.read_pickle('data/customers.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'General population (azidas) has {azdias.shape[0]} rows and {azdias.shape[1]} columns')\n",
    "print(f'Customer population (customers) has {customers.shape[0]} rows and {customers.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the datasets were loaded correctly. Lets begin this exploration phase with a look at the summary statistics for all continuous variables as well as the first couple of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that is pretty obvious by just looking at the tables above is that some columns seem to contain a lot of NaN:s. The descriptive summary below shows this as well.\n",
    "\n",
    "The amount of missing values will increase after converting missing value codes/unknowns to NaN:s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing\n",
    "In this section I'll investigate and handle missing values, re-encode features, some feature engineering as well as feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute</th>\n",
       "      <th>information_level</th>\n",
       "      <th>type</th>\n",
       "      <th>missing_or_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>person</td>\n",
       "      <td>categorical</td>\n",
       "      <td>[-1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALTERSKATEGORIE_GROB</td>\n",
       "      <td>person</td>\n",
       "      <td>ordinal</td>\n",
       "      <td>[-1, 0, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANREDE_KZ</td>\n",
       "      <td>person</td>\n",
       "      <td>categorical</td>\n",
       "      <td>[-1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CJT_GESAMTTYP</td>\n",
       "      <td>person</td>\n",
       "      <td>categorical</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FINANZ_MINIMALIST</td>\n",
       "      <td>person</td>\n",
       "      <td>ordinal</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              attribute information_level         type missing_or_unknown\n",
       "0              AGER_TYP            person  categorical            [-1, 0]\n",
       "1  ALTERSKATEGORIE_GROB            person      ordinal         [-1, 0, 9]\n",
       "2             ANREDE_KZ            person  categorical            [-1, 0]\n",
       "3         CJT_GESAMTTYP            person  categorical                [0]\n",
       "4     FINANZ_MINIMALIST            person      ordinal               [-1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_info = pd.read_csv('data/feat_final.csv', sep=',')\n",
    "feat_info.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "feat_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features information file loaded above contains valuable information about each feature. This file will be used throughout the project to help decide what needs to be done and how to do it.\n",
    "\n",
    "The feature file is a combination of the old feature file from the unsupervised project in term 1 and the new feature file. To create the feature file used in this project I had to do the following:\n",
    "\n",
    "- Manually scan through the feature and the attribute file provided for this project and:\n",
    "\t- Assign feature type\n",
    "\t- Assign missing or unknown codes\n",
    "        - The values for ``'unknown'``, ``'unknown / no main age detectable'``, ``'no transactions known'`` or ``'no transaction known'`` where used to define these codes.\n",
    "- Check for contradicting information\n",
    "\t- Some features that were found in both the file from term 1 and the one provided for this project have different values for the missing or unknown column.  One example of this is the codes for the feature ``CAMEO_DEUG_2015``. The old feature file states that ``[-1, X]`` should be treated as missing while the new only contains ``[-1]``. For cases like this I decided to go with the values from the file from term 1. The reason for this is simply that this information is corresponding with the values in the dataset better.\n",
    "- Convert all values in the missing_or_unknown column to abide to the following format:\n",
    "    - [int] --> [1]\n",
    "    - [int, int] --> [-1, 0]\n",
    "    - [int, 'str'] --> [1, 'XX']\n",
    "    - [] --> []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias['CAMEO_DEUG_2015'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though most of the creation of the feature file went smooth some features (57 to be exact) found in the dataset could not be found in any of the provided feature files. This is a problem due to the fact that we have no information about what type of feature these are or what values in those columns that should be treated as missing. I decided to treat all these values as ordinal and assigned an empty array to the missing or unknown column. Another solution would be to just drop all these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Assess Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values per column before converting missing codes\n",
    "missing_values_unparsed = azdias.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert missing value codes to NaN\n",
    "azdias_parsed = convert_missing_codes(azdias, feat_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values per column after converting missing codes\n",
    "missing_values_parsed = azdias_parsed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_before_total = missing_values_unparsed.sum()\n",
    "missing_after_total = missing_values_parsed.sum()\n",
    "diff = missing_after_total - missing_before_total\n",
    "\n",
    "print(f'Number of missing values before converstion: {missing_before_total}')\n",
    "print(f'Number of missing values after converstion: {missing_after_total}')\n",
    "print('Increase in missing values: {0:.2f} % '.format(diff / missing_before_total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting so feat_info.... is in the same order as other dfs\n",
    "sorter = missing_values_unparsed.index\n",
    "sorterIndex = dict(zip(sorter, range(len(sorter))))\n",
    "feat_info_final_sorted = feat_info.copy()\n",
    "feat_info_final_sorted['feature_id_rank'] = feat_info_final_sorted['attribute'].map(sorterIndex)\n",
    "feat_info_final_sorted.sort_values('feature_id_rank', ascending=True, inplace=True)\n",
    "feat_info_final_sorted = feat_info_final_sorted.drop('feature_id_rank', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_missing_values = pd.DataFrame(\n",
    "    {\n",
    "        'Before': missing_values_unparsed,\n",
    "        'After': missing_values_parsed,\n",
    "        'Diff': missing_values_parsed - missing_values_unparsed,\n",
    "        'Percentage_Missing': missing_values_parsed / azdias.shape[0] * 100,\n",
    "        'Information_Level': feat_info_final_sorted['information_level'].values\n",
    "    })\n",
    "\n",
    "df_missing_values.sort_values([\"After\", \"Diff\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the table above it's clear that some features have a lot more missing values after converting the missing value codes to NaN:s. Many of the columns that got a substantial increase in NaN's are belonging to ``D19`` (transactional). One thing that they have in common is that they have either \"0\" or \"10\" as missing value codes. In the documentation, these values are classified as \"no transactions known\" and as pointed out earlier I classified this as meaning missing or unknown.\n",
    "\n",
    "There are probably few columns that are outliers in terms of the proportion of values that are missing. Matplotlib's hist() function will be used visualize the distribution of missing value counts to find these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.hist(df_missing_values['Percentage_Missing'], bins=np.linspace(0,100,21))\n",
    "plt.title('Distribution of Missing Values per Feature');\n",
    "plt.xticks(np.linspace(0,100,21))\n",
    "plt.xlabel('% Missing Values')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "less_30 = df_missing_values[df_missing_values['Percentage_Missing'] <= 30]['Percentage_Missing']\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "less_30.hist()\n",
    "plt.title('Distribution of Missing Values per Column <= to 30%');\n",
    "plt.xlabel('% Missing Values')\n",
    "plt.ylabel('Number of Features');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_outliers = df_missing_values[df_missing_values['Percentage_Missing'] > 30]\n",
    "print(f'{missing_outliers.shape[0]} features have more than 30 % missing values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before removing any features lets see if anything interesting can be observed by plotting the amount of missing values per feature grouped by information_level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_information_levels(df):\n",
    "    \"\"\"\n",
    "    Plot every feature within each information_level group.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe) - Dataframe to be used\n",
    "    \"\"\"\n",
    "    sns.set(style='whitegrid')\n",
    "    sns.set_color_codes('pastel')\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    information_level = df['Information_Level'].unique()\n",
    "    for idx, level in enumerate(information_level):\n",
    "        sns.set()\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.set(style=\"ticks\")\n",
    "        features = df[df['Information_Level'] == level].sort_values(['After'], ascending=False)\n",
    "        ax = sns.barplot(x=features.index.values, y='After', data=features)\n",
    "        ax.set(xlabel=\"Feature\", ylabel=\"# Missing Values\", title=level.title())\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "        fig.set_size_inches(22,7)\n",
    "            \n",
    "\n",
    "plot_information_levels(df_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group features by number of missing values and print groups that contain more than one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict key = # missing values and value = array of features\n",
    "missing_sorted = missing_values_parsed.sort_values()\n",
    "missing_groups = dict()\n",
    "for feat, val in missing_sorted.items():\n",
    "    if val in missing_groups:\n",
    "        missing_groups[val].append(feat)\n",
    "    else:\n",
    "        missing_groups[val] = [feat]\n",
    "\n",
    "for val, feat in missing_groups.items():\n",
    "    if len(feat) > 1:     \n",
    "        print(f'These features share {val} missing value:\\n {feat} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the plots and output above I found that features similar features or features that describe similar information but at different scales often have the same amount of missing values. Let's take ``LP_FAMILIE_FEIN`` and ``LP_FAMILIE_GROB`` for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_values.loc[['LP_FAMILIE_FEIN', 'LP_FAMILIE_GROB']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both describe the person's family type, but at different scales. Fein translates to fine and Grob to rough. It's also stated that values in ``LP_FAMILIE_FEIN`` map to values in ``LP_FAMILIE_GROB``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Removing Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis have shown that the distribution of missing values per feature are skewed to the right meaning most features have zero or a relative small amount of missing values. The majority of the features have less than 30 % missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_top_missing = [x for x in missing_outliers.index]\n",
    "display_all(df_missing_values[df_missing_values.index.isin(features_top_missing)].sort_values('Percentage_Missing', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_feat_file = pd.read_csv('data/AZDIAS_Feature_Summary.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_exclusive = np.setdiff1d(features_top_missing, old_feat_file['attribute'])\n",
    "\n",
    "print(f'Features with more than 30 % missing values originating from old feature file: {len(features_top_missing) - len(new_exclusive)}')\n",
    "print(f'Features with more than 30 % missing values originating from new feature file: {len(new_exclusive)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear pattern can be observed for the features with the largest amount of missing values. Many of the features that got a large increase in missing values after the conversion are classified as either household or 125x125_grid. Another observation is that 60 out of the 66 features are from the new feature file. Why this is the case is hard to answer, but it could be because I manually parsed the missing code values (most are either 0 or 10).\n",
    "\n",
    "Deciding which features to remove from the analysis is difficult. Rather than just using a threshold we should also try to take into account the amount of information we think the features contain. Another important question to ask is why do we have missing values? Is it due to errors, data gathering technique or do we have missing at random, missing completely at random or missing not at random values?\n",
    "\n",
    "The sparse information regarding each feature in combination with the lack of domain knowledge as well as not knowing the German language makes this almost impossible and for that reason I have decided to use a threshold of 30 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_missing = df_missing_values[df_missing_values['Percentage_Missing'] == 0]\n",
    "# skip index\n",
    "for feat in no_missing[1:].index:\n",
    "    info = feat_info[feat_info['attribute'] == feat]['information_level'].values\n",
    "    print(f'{feat} is of information_type: {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features that have no missing values are of type person or other. ``Other`` is the information_level I added to those features that had no information available. It's possible that I have classified some features wrong which makes it fairly possible that all features that have no missing values are actually of type person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with 30 % or or missing values\n",
    "azdias_feat_dropped = azdias_parsed.drop(features_top_missing, axis=1)\n",
    "azdias_feat_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Removing Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "azdias_feat_dropped.isnull().sum(axis=1).hist(bins=20)\n",
    "plt.title('Missing Values per Row');\n",
    "plt.xlabel('# Features')\n",
    "plt.ylabel('# Rows');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use a threshold of 30 % which means that rows that have more than 90 features missing will be treated as outliners and dropped. These rows will be placed in a separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_low_missing, azdias_high_missing = split_df(azdias_feat_dropped, 210)\n",
    "\n",
    "azdias_low_missing.shape, azdias_high_missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset with small amount of missing values per row has {azdias_low_missing.shape[0]} rows')\n",
    "print(f'Dataset with large amount of missing values per row has {azdias_high_missing.shape[0]} rows')\n",
    "print(f'New dataset contains {azdias_low_missing.shape[0] / azdias_feat_dropped.shape[0] * 100} % of the original amount of rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ZABEOTYP', 'SEMIO_TRADV', 'FINANZ_VORSORGER', 'FINANZTYP', 'ALTERSKATEGORIE_GROB', 'ONLINE_AFFINITAET']\n",
    "\n",
    "for column in columns:\n",
    "    compare_columns([azdias_low_missing, azdias_high_missing], column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the dataset with a high amount of missing values only makes up around 12 % of the total population we should pay close attention to this group. As shown above the distribution for some features are different between the two groups, which suggest that these groups are quite different from each other.\n",
    "\n",
    "From here on I'll focus the analysis on the group with the smaller amount of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Feature Engineering\n",
    "\n",
    "Handeling missing values is not the only preprocessing that needs to be done. Since the unsupervised learning algorithms that will be used in the customer segmentation report only work on data that is encoded numerically, all none numerical values must be encoded to numerical. Even though almost all of the values are numerical some of them don't represent numeric values (categories).\n",
    "\n",
    "The type column in the feature file will be used as guidance in respect to each feature type:\n",
    "- Interval and numeric features will be left as they are\n",
    "- Most of the variables in the dataset are ordinal in nature. While ordinal values may technically be non-linear in spacing, make the simplifying assumption that the ordinal variables can be treated as being interval in nature (that is, kept without any changes).\n",
    "- Special handling may be necessary for the remaining two variable types: categorical, and 'mixed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_info_300 = feat_info[feat_info['attribute'].isin(azdias_low_missing.columns)].copy()\n",
    "feat_info_300['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_features = azdias_low_missing.select_dtypes(include=['object']).columns\n",
    "for feat in obj_features:\n",
    "    feat_type = feat_info_300[feat_info_300['attribute'] == feat]['type'].values\n",
    "    if feat_type not in ['mixed', 'categorical']:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_info_300.loc[feat_info_300['attribute'] == 'D19_LETZTER_KAUF_BRANCHE', 'type'] = 'categorical'\n",
    "feat_info_300.loc[feat_info_300['attribute'] == 'EINGEFUEGT_AM', 'type'] = 'mixed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D19_LETZTER_KAUF_BRANCHE and EINGEFUEGT_AM are both features that are of type object. I've decided to classifiy D19_LETZTER_KAUF_BRANCHE as categorical and EINGEFUEGT_AM as mixed and they will be re-encoded with their corresponding type down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = get_feature_types(azdias_low_missing, feat_info_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Remaining categorical features in the dataset:\\n {types['categorical']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary, multiple = categorical_info(azdias_low_missing, types['categorical'])\n",
    "print('Binary Features:')\n",
    "print('\\n'.join('Column: {} Values: {}'.format(k, v) for k, v in binary.items()))\n",
    "print('-' * 35)\n",
    "print('Multi-Level Features:')\n",
    "print('\\n'.join('Column: {} Values: {}'.format(k, v) for k, v in multiple.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in multiple.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Remaining mixed features in the dataset:\\n {types['mixed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above the ordinal and interval features will be treated as raw numeric values meaning they will be left alone. With regards to the categorical and mixed features I've decided to do the following:\n",
    "\n",
    "- ``OST_WEST_KZ`` is the only binary categorical feature that needs to be re-encoded.\n",
    "- ``WOHNLAGE`` will be treated as categorical and re-encoded with dummy.\n",
    "- ``CAMEO_DEU_2015`` and ``CAMEO_DEUG_2015`` are both describing the wealth and life stage topology but at different scales. I've decided to keep ``CAMEO_DEUG_2015`` which describes the information at a rough scale and drop ``CAMEO_DEU_2015``. Another reason for dropping this feature is that it contains over 40 categories.\n",
    "- ``CAMEO_INTL_2015`` combines information on two axes: wealth and life stage. Out of this feature two new ordinal features will be created. This will be achieved by splitting the original feature by it's tens- and ones-place.\n",
    "- ``PRAEGENDE_JUGENDJAHRE`` combines information on three dimensions: generation by decade, movement (mainstream vs. avantgarde), and nation (east vs. west). While there aren't enough levels to disentangle east from west, two new variables will be created to capture the other two dimensions: an interval-type variable for decade, and a binary variable for movement.\n",
    "- ``EINGEFUEGT_AM`` is in date format and will be re-encoded to capture the year.  \n",
    "- ``PLZ8_BAUMAX`` will be treated as interval (no change)\n",
    "- ``KBA05_HERSTTEMP`` and ``KBA05_MODTEMP`` will also be treated as interval (no change)\n",
    "- ``LP_LEBENSPHASE_FEIN`` and ``LP_LEBENSPHASE_GROB`` will be dropped due to being high dimensional and to hard to separate in a good way.\n",
    "- ``LNR`` is some kind of id/index and will be dropped.\n",
    "\n",
    "I'll create a clean_data function that will be used to apply the re-enconding above and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Step 1 - Load *****\n",
      "Preparing to clean dataset with shape: (891221, 366)\n",
      "\n",
      "Dropped these features from dataframe and feat_info: \n",
      "['LNR', 'LP_LEBENSPHASE_FEIN', 'LP_LEBENSPHASE_GROB', 'CAMEO_DEU_2015']\n",
      "Shape after Step 1 - Load: (891221, 362)\n",
      "\n",
      "****** Step 2 - Convert NaN codes *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------------------------------] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Step 2 - Convert NaN codes: (891221, 362)\n",
      "\n",
      "****** Step 3 - Drop Columns with Missing values >= 30 % *****\n",
      "Shape after Step 3 - Drop Columns with Missing values >= 30 %: (891221, 296)\n",
      "\n",
      "****** Step 4 - Drop Rows with Missing values >= 30 % *****\n",
      "Shape after Step 4 - Drop Rows with Missing values >= 30 %: (785215, 296)\n",
      "\n",
      "****** Step 5 - Impute Missing Values *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------------------------------] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Step 5 - Impute Missing Values: (785215, 296)\n",
      "\n",
      "AM NULL: 0\n",
      "****** Step 6 - Re-encode Binary Categorical Features *****\n",
      "Shape after Step 6 - Re-encode Binary Categorical Features: (785215, 296)\n",
      "\n",
      "****** Step 7 - Re-encode Multi-Level Categorical Features *****\n",
      "Shape after Step 7 - Re-encode Multi-Level Categorical Features: (785215, 422)\n",
      "\n",
      "****** Step 8 - Re-encode Mixed Features *****\n",
      "Shape after Step 8 - Re-encode Mixed Features: (785215, 424)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "azdias_cleaned = clean_data(azdias, feat_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#azdias_cleaned.to_pickle('data/azdias_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_cleaned = clean_data(customers, feat_info, azdias_cleaned.columns, customer_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers_cleaned.to_pickle('data/customers_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle files\n",
    "azdias_cleaned = pd.read_pickle('data/azdias_cleaned.pkl')\n",
    "customers_cleaned = pd.read_pickle('data/customers_cleaned.pkl')\n",
    "\n",
    "azdias_cleaned.shape, customers_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature Scaling\n",
    "Before applying dimensional reduction techniques to the data, feature scaling needs to be done so that the principal component vectors are not influenced by the natural differences in scale for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# fit transform\n",
    "scaled = scaler.fit_transform(azdias_cleaned.astype(float))\n",
    "\n",
    "azdias_scaled = pd.DataFrame(data=scaled, index=azdias_cleaned.index, columns=azdias_cleaned.columns)\n",
    "print(\"original shape:   \", azdias_cleaned.shape)\n",
    "print(\"scaled shape:\", azdias_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler_file = 'models/scaler.save'\n",
    "#joblib.dump(scaler, scaler_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Perform Dimensionality Reduction\n",
    "\n",
    "With the data scaled, we are now ready to apply dimensionality reduction techniques.\n",
    "\n",
    "- Sklearn's PCA class will be used to perform principal component analysis on the data, thus finding the vectors of maximal varaiance in the data. To start with all the features will be kept (so there's enough features to see the general trend in variability).\n",
    "- Both the ratio of variance explained by each component as well as the cumulative variance explained will be plotted.\n",
    "- Re-evaluate the number of components to keep and then re-fit the transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca = PCA()\n",
    "\n",
    "# fit transform\n",
    "azdias_pca = pca.fit_transform(azdias_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the variance accounted for by each principal component.\n",
    "variance = pca.explained_variance_ratio_\n",
    "\n",
    "cumsum_variance=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = len(pca.explained_variance_ratio_)\n",
    "index = np.arange(n_components)\n",
    "vals = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(13,15))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.bar(index, cumsum_variance)\n",
    "plt.ylabel('% Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.linspace(0,500, 10, endpoint=False))\n",
    "plt.title('PCA Analysis')\n",
    "\n",
    "# 15 components\n",
    "plt.hlines(y=30, xmin=0, xmax=15, color='red', linestyles='-',zorder=1)\n",
    "plt.vlines(x=15, ymin=0, ymax=30, color='red', linestyles='-',zorder=2)\n",
    "\n",
    "# 100 components\n",
    "plt.hlines(y=61, xmin=0, xmax=100, color='red', linestyles='-',zorder=3)\n",
    "plt.vlines(x=100, ymin=0, ymax=61, color='red', linestyles='-',zorder=4)\n",
    "\n",
    "# 200\n",
    "plt.hlines(y=81, xmin=0, xmax=200, color='red', linestyles='-',zorder=5)\n",
    "plt.vlines(x=200, ymin=0, ymax=81, color='red', linestyles='-',zorder=6)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(index, vals)\n",
    "plt.xticks(np.linspace(0,500, 10, endpoint=False))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('% Explained Cariance')\n",
    "plt.title('PCA Analysis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show that the explained variance plateaus a couple of times. The first and sharpest drop-off is around 15 components. At 15 componenets around 30 % of the total variance is explained. After this we can observe two other, but not as steep drop-offs around 100 and 200 components. At 100 components 61 % of the total variance is explained and at 200 82 %.\n",
    "\n",
    "With each consecutive component less and less variance is explained which means that after a while the amount of additional explained variance probably is not worth the extra number of components.\n",
    "\n",
    "With that said 100 components could be considered a good middle point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "print(f'{n_components} has a cumulative explained variane of: {sum(pca.explained_variance_ratio_[:n_components])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-fit with n components\n",
    "# apply PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# fit transform\n",
    "azdias_pca = pca.fit_transform(azdias_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca_file = 'models/pca.save'\n",
    "#joblib.dump(pca, pca_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Interpret Principal Components\n",
    "Now that we have our transformed principal components, it's a good idea to check out the weight of each variable on the first few components to see if they can be interpreted in some fashion.\n",
    "\n",
    "Each principal component is a unit vector that points in the direction of highest variance (after accounting for the variance captured by earlier principal components). The further a weight is from zero, the more the principal component is in the direction of the corresponding feature. If two features have large weights of the same sign (both positive or both negative), then increases in one tend expect to be associated with increases in the other. To contrast, features with different signs can be expected to show a negative correlation: increases in one variable should result in a decrease in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1 = interpret_pca(azdias_scaled, pca, 0)\n",
    "display_all(dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(dim1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim2 = interpret_pca(azdias_scaled, pca, 1)\n",
    "display_all(dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(dim2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim3 = interpret_pca(azdias_scaled, pca, 2)\n",
    "display_all(dim3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_pca(dim3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Apply Clustering to General Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Number of different cluster counts\n",
    "n_clusters = np.arange(2,31)\n",
    "\n",
    "# run k-means clustering on the data and compute the average within-cluster distances.\n",
    "scores = [MiniBatchKMeans(i).fit(azdias_pca).score(azdias_pca) for i in n_clusters]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow method\n",
    "plt.ylabel('SSE')\n",
    "plt.xlabel('# Clusters')\n",
    "plt.title('SSE vs Clusters')\n",
    "plt.plot(n_clusters, np.abs(scores), linestyle='-', marker='o', color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# re-fith with 13 clusters\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "#kmeans = KMeans(n_clusters=11, random_state=42, n_jobs=-1)\n",
    "\n",
    "# general population predictions\n",
    "azdias_predictions = kmeans.fit_predict(azdias_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans_file = 'models/kmeansstrange.save'\n",
    "#joblib.dump(kmeans, kmeans_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scaler, pca and kmeans\n",
    "customers_scaled = scaler.transform(customers_cleaned.astype('float'))\n",
    "customers_pca = pca.transform(customers_scaled)\n",
    "customers_preds = kmeans.predict(customers_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_predictions = clusters_predict(azdias_cleaned)\n",
    "customers_preds = clusters_predict(customers_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load('models/scaler.save')\n",
    "pca = joblib.load('models/pca.save')\n",
    "kmeans = joblib.load('models/kmeans.save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Compare Customer Data to Demographics Data \n",
    "\n",
    "At this point, we have clustered data based on demographics of the general population of Germany, and seen how the customer data for a mail-order sales company maps onto those demographic clusters. Let's end this section with a comparison of the two cluster distributions to see where the strongest customer base for the company is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general_proportions = Counter(azdias_predictions['Clusters'])\n",
    "#customers_proportions = Counter(customers_preds['Clusters'])\n",
    "\n",
    "general_proportions = Counter(azdias_predictions)\n",
    "customers_proportions = Counter(customers_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in general_proportions.items():\n",
    "    general_proportions[k] = v / len(azdias)\n",
    "    \n",
    "for k, v in customers_proportions.items():\n",
    "    customers_proportions[k] = v / len(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_props = pd.DataFrame(columns=[\"Dataset\", \"Cluster\", \"Proportions\"])\n",
    "for k, v in customers_proportions.items():\n",
    "    df_props = df_props.append({\"Dataset\" : \"customer\", \"Cluster\": k, \"Proportions\": v}, ignore_index=True)\n",
    "\n",
    "for k, v in general_proportions.items():\n",
    "    df_props = df_props.append({\"Dataset\" : \"general\", \"Cluster\": k, \"Proportions\": v}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "sns.barplot(x=\"Cluster\", y=\"Proportions\", hue=\"Dataset\", data=df_props);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_target = kmeans.cluster_centers_[5]\n",
    "center_general = kmeans.cluster_centers_[4]\n",
    "\n",
    "center_target.shape, center_general.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to large scale in some of the features no reversing of the scaler is applied.\n",
    "# Would skew the plot\n",
    "center_target_inv = pca.inverse_transform(center_target)\n",
    "center_general_inv = pca.inverse_transform(center_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = azdias_cleaned.columns\n",
    "\n",
    "plt.figure(figsize=(10,100))\n",
    "\n",
    "sns.barplot(x=center_target_inv, y=features, label=\"C5 Target\", color=\"#5499C7\")\n",
    "sns.barplot(x=center_general_inv, y=features, label=\"C4 General\", color=\"#E59866\")\n",
    "          \n",
    "plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(column, customer, general):\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(12,4), ncols=2)\n",
    "    sns.countplot(x = column, data=customer, ax=ax1, palette=\"Set3\")\n",
    "    ax1.set_xlabel('Value')\n",
    "    ax1.set_title('Distribution of target population')\n",
    "    sns.countplot(x = column, data=general, ax=ax2, palette=\"Set3\")\n",
    "    ax2.set_xlabel('Value')\n",
    "    ax2.set_title('Distribution of non-target population')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_azdias = pd.DataFrame(azdias_predictions['Clusters'].values, columns = ['Cluster'])\n",
    "df_preds_customers = pd.DataFrame(customers_preds['Clusters'].values, columns = ['Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_preds_customers[df_preds_customers['Cluster'] == 5].index\n",
    "general = df_preds_azdias[df_preds_azdias['Cluster'] == 4].index\n",
    "\n",
    "df_target = customers_cleaned.iloc[target]\n",
    "df_general = azdias_cleaned.iloc[general]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_comparison('HH_EINKOMMEN_SCORE', df_target, df_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison('KKK', df_target, df_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison('MOBI_REGIO', df_target, df_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison('ANREDE_KZ', df_target, df_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison('EWDICHTE', df_target, df_general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Discussing Compare Customer and Demographics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are over 400 features in the dataset it's impracticable to compare the datasets exhaustively. Therefore, I've decided to compare five features with the purpose to illustrate a point.\n",
    "\n",
    "- ``HH_EINKOMMEN_SCORE``: estimated household net income (lower = higher income)\n",
    "- ``KKK``: purchasing power (lower = higher purchasing power)\n",
    "- ``MOBI_REGIO``: moving patterns (lower = higher mobility)\n",
    "- ``ANREDE_KZ``: gender (1 = male & 2 = female)\n",
    "- ``EWDICHTE``: density of inhabitants per square kilometer (lower = less dense)\n",
    "\n",
    "From the tables above we can observe that the customer population have a higher estimated household net income and higher purchasing power. Customers are less mobile, a larger proportion are male and are living in less dense areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42962, 367), (42833, 366))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mailout_train = pd.read_csv('data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';', dtype={18:'str',19:'str'})\n",
    "mailout_test = pd.read_csv('data/Udacity_MAILOUT_052018_TEST.csv', sep=';', dtype={18:'str',19:'str'})\n",
    "\n",
    "mailout_train.shape, mailout_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42962, 367), (42833, 366))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mailout_train.to_pickle('data/mailout_train.pkl')\n",
    "#mailout_test.to_pickle('data/mailout_test.pkl')\n",
    "\n",
    "mailout_train = pd.read_pickle('data/mailout_train.pkl')\n",
    "mailout_test = pd.read_pickle('data/mailout_test.pkl')\n",
    "mailout_train.shape, mailout_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = mailout_train['RESPONSE']\n",
    "mailout_train.drop('RESPONSE', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Step 1 - Load *****\n",
      "Preparing to clean dataset with shape: (42962, 366)\n",
      "\n",
      "Dropped these features from dataframe and feat_info: \n",
      "['LNR', 'LP_LEBENSPHASE_FEIN', 'LP_LEBENSPHASE_GROB', 'CAMEO_DEU_2015']\n",
      "Shape after Step 1 - Load: (42962, 362)\n",
      "\n",
      "****** Step 2 - Convert NaN codes *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------------------------------] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Step 2 - Convert NaN codes: (42962, 362)\n",
      "\n",
      "****** Step 3 - Drop Columns with Missing values >= 30 % *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Step 3 - Drop Columns with Missing values >= 30 %: (42962, 296)\n",
      "\n",
      "****** Step 4 - Drop Rows with Missing values >= 30 % *****\n",
      "Shape after Step 4 - Drop Rows with Missing values >= 30 %: (42962, 296)\n",
      "\n",
      "****** Step 5 - Impute Missing Values *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------------------------------] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Step 5 - Impute Missing Values: (42962, 296)\n",
      "\n",
      "AM NULL: 0\n",
      "****** Step 6 - Re-encode Binary Categorical Features *****\n",
      "Shape after Step 6 - Re-encode Binary Categorical Features: (42962, 296)\n",
      "\n",
      "****** Step 7 - Re-encode Multi-Level Categorical Features *****\n",
      "Shape after Step 7 - Re-encode Multi-Level Categorical Features: (42962, 421)\n",
      "\n",
      "****** Step 8 - Re-encode Mixed Features *****\n",
      "Shape after Step 8 - Re-encode Mixed Features: (42962, 423)\n",
      "\n",
      "****** Step 8 - Add Missing Columns *****\n",
      "Shape after Step 8 - Add Missing Columns: (42962, 424)\n",
      "\n",
      "****** Step 1 - Load *****\n",
      "Preparing to clean dataset with shape: (42833, 366)\n",
      "\n",
      "Dropped these features from dataframe and feat_info: \n",
      "['LNR', 'LP_LEBENSPHASE_FEIN', 'LP_LEBENSPHASE_GROB', 'CAMEO_DEU_2015']\n",
      "Shape after Step 1 - Load: (42833, 362)\n",
      "\n",
      "****** Step 2 - Convert NaN codes *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------------------------------] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Step 2 - Convert NaN codes: (42833, 362)\n",
      "\n",
      "****** Step 3 - Drop Columns with Missing values >= 30 % *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Step 3 - Drop Columns with Missing values >= 30 %: (42833, 296)\n",
      "\n",
      "****** Step 4 - Drop Rows with Missing values >= 30 % *****\n",
      "Shape after Step 4 - Drop Rows with Missing values >= 30 %: (42833, 296)\n",
      "\n",
      "****** Step 5 - Impute Missing Values *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------------------------------] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Step 5 - Impute Missing Values: (42833, 296)\n",
      "\n",
      "AM NULL: 0\n",
      "****** Step 6 - Re-encode Binary Categorical Features *****\n",
      "Shape after Step 6 - Re-encode Binary Categorical Features: (42833, 296)\n",
      "\n",
      "****** Step 7 - Re-encode Multi-Level Categorical Features *****\n",
      "Shape after Step 7 - Re-encode Multi-Level Categorical Features: (42833, 421)\n",
      "\n",
      "****** Step 8 - Re-encode Mixed Features *****\n",
      "Shape after Step 8 - Re-encode Mixed Features: (42833, 423)\n",
      "\n",
      "****** Step 8 - Add Missing Columns *****\n",
      "Shape after Step 8 - Add Missing Columns: (42833, 424)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mailout_train_cleaned = clean_data(mailout_train, feat_info, columns=azdias_cleaned.columns, drop_rows=False)\n",
    "mailout_test_cleaned = clean_data(mailout_test, feat_info, columns=azdias_cleaned.columns, drop_rows=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(target).set_title(\"Distribution of Label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the table above the dataset is imbalanced: there are far more observations where the person has not responded (0) than responsed (1). Therefore, accuracy is a poor mertric to use. The Receiver Operating Characteristic Area Under the Curve (ROC AUC) will be used instead. Randomly guessing will lead to a ROC AUC of 0.5 and a perfect score would be 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation dataset.\n",
    "# use stratify to make sure same proportions of class labels as the input dataset\n",
    "\n",
    "SEED = 28\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mailout_train_cleaned.astype('float'),\n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=SEED,\n",
    "                                                    stratify = target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set will be used to validate the model. This part of the data will not be used to train on when tuning hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Approach and Default Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what the initial analysis show I may try to get a better score by creating a stacked model (ml-ens). This approach will hopefully lead to a better prediction. In addition to this I'll use the skopt library to tune the hyperparameters (Bayesian optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 stratified folds\n",
    "skf = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "skf.get_n_splits(mailout_train_cleaned, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_models():\n",
    "    basedModels = []\n",
    "    basedModels.append(('LR', LogisticRegression(solver='liblinear', random_state=SEED)))\n",
    "    basedModels.append(('RF', RandomForestClassifier(n_estimators=100, random_state=SEED)))\n",
    "    basedModels.append(('XGB', xgb.XGBClassifier(random_state=SEED)))\n",
    "    basedModels.append(('LGBM', lgb.LGBMClassifier(random_state=SEED)))\n",
    "    basedModels.append(('GB', GradientBoostingClassifier(random_state=SEED)))\n",
    "    basedModels.append(('MLP', MLPClassifier(random_state=SEED)))\n",
    "    \n",
    "    return basedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(features, target, models):\n",
    "    results = []\n",
    "    names = []\n",
    "    for name, model in models:\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        \n",
    "    return names, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_preprocess(type_of_scaler):\n",
    "    \n",
    "    if type_of_scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif type_of_scaler == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "    pipelines = []\n",
    "    pipelines.append((type_of_scaler+'LR', Pipeline([('Scaler', scaler), ('LR', LogisticRegression(solver='liblinear', random_state=SEED))])))\n",
    "    pipelines.append((type_of_scaler+'RF', Pipeline([('Scaler', scaler), ('RF', RandomForestClassifier(n_estimators=100, random_state=SEED))])))   \n",
    "    pipelines.append((type_of_scaler+'XGB', Pipeline([('Scaler', scaler), ('XGB', xgb.XGBClassifier(random_state=SEED))])))\n",
    "    pipelines.append((type_of_scaler+'LGBM', Pipeline([('Scaler', scaler), ('LGBM', lgb.LGBMClassifier(random_state=SEED))])))\n",
    "    pipelines.append((type_of_scaler+'GB', Pipeline([('Scaler', scaler), ('GB', GradientBoostingClassifier(random_state=SEED))])))   \n",
    "    pipelines.append((type_of_scaler+'MLP', Pipeline([('Scaler', scaler), ('MLP', MLPClassifier(random_state=SEED))])))   \n",
    "\n",
    "    \n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score_df(names, results):\n",
    "    def floatingDecimals(f_val, dec=3):\n",
    "        prc = \"{:.\"+str(dec)+\"f}\" \n",
    "    \n",
    "        return float(prc.format(f_val))\n",
    "\n",
    "    scores = []\n",
    "    for r in results:\n",
    "        scores.append(floatingDecimals(r.mean(),4))\n",
    "\n",
    "    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n",
    "    return scoreDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking how the models perform on unscaled, standard scaled and minmax scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.665530 (0.036172)\n",
      "RF: 0.597039 (0.022422)\n",
      "XGB: 0.755744 (0.029427)\n",
      "LGBM: 0.707587 (0.013561)\n",
      "GB: 0.759239 (0.022813)\n",
      "MLP: 0.607565 (0.056376)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.6655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.5970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.7557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.7076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GB</td>\n",
       "      <td>0.7592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.6076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model   Score\n",
       "0    LR  0.6655\n",
       "1    RF  0.5970\n",
       "2   XGB  0.7557\n",
       "3  LGBM  0.7076\n",
       "4    GB  0.7592\n",
       "5   MLP  0.6076"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unscaled \n",
    "models = create_base_models()\n",
    "names, results = evaluate(mailout_train_cleaned, target, models)\n",
    "baseline_score = create_score_df(names, results)\n",
    "baseline_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardLR: 0.624120 (0.026171)\n",
      "standardRF: 0.562346 (0.021686)\n",
      "standardXGB: 0.760572 (0.027826)\n",
      "standardLGBM: 0.689190 (0.018108)\n",
      "standardGB: 0.752448 (0.015501)\n",
      "standardMLP: 0.574412 (0.036794)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.6385</td>\n",
       "      <td>standardLR</td>\n",
       "      <td>0.6241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.5743</td>\n",
       "      <td>standardRF</td>\n",
       "      <td>0.5623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.7589</td>\n",
       "      <td>standardXGB</td>\n",
       "      <td>0.7606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>standardLGBM</td>\n",
       "      <td>0.6892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GB</td>\n",
       "      <td>0.7481</td>\n",
       "      <td>standardGB</td>\n",
       "      <td>0.7524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>standardMLP</td>\n",
       "      <td>0.5744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model   Score         Model   Score\n",
       "0    LR  0.6385    standardLR  0.6241\n",
       "1    RF  0.5743    standardRF  0.5623\n",
       "2   XGB  0.7589   standardXGB  0.7606\n",
       "3  LGBM  0.7005  standardLGBM  0.6892\n",
       "4    GB  0.7481    standardGB  0.7524\n",
       "5   MLP  0.5790   standardMLP  0.5744"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard scaler\n",
    "models = get_scaled_preprocess('standard')\n",
    "names, results = evaluate(X_train, y_train, models)\n",
    "standard_scaler_score = create_score_df(names, results)\n",
    "\n",
    "df_score = pd.concat([baseline_score, standard_scaler_score], axis=1)\n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmaxLR: 0.652879 (0.016759)\n",
      "minmaxRF: 0.576846 (0.036397)\n",
      "minmaxXGB: 0.760824 (0.024929)\n",
      "minmaxLGBM: 0.694179 (0.020771)\n",
      "minmaxGB: 0.753639 (0.018721)\n",
      "minmaxMLP: 0.576122 (0.024516)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.6385</td>\n",
       "      <td>standardLR</td>\n",
       "      <td>0.6241</td>\n",
       "      <td>minmaxLR</td>\n",
       "      <td>0.6529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.5743</td>\n",
       "      <td>standardRF</td>\n",
       "      <td>0.5623</td>\n",
       "      <td>minmaxRF</td>\n",
       "      <td>0.5768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.7589</td>\n",
       "      <td>standardXGB</td>\n",
       "      <td>0.7606</td>\n",
       "      <td>minmaxXGB</td>\n",
       "      <td>0.7608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>standardLGBM</td>\n",
       "      <td>0.6892</td>\n",
       "      <td>minmaxLGBM</td>\n",
       "      <td>0.6942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GB</td>\n",
       "      <td>0.7481</td>\n",
       "      <td>standardGB</td>\n",
       "      <td>0.7524</td>\n",
       "      <td>minmaxGB</td>\n",
       "      <td>0.7536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>standardMLP</td>\n",
       "      <td>0.5744</td>\n",
       "      <td>minmaxMLP</td>\n",
       "      <td>0.5761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model   Score         Model   Score       Model   Score\n",
       "0    LR  0.6385    standardLR  0.6241    minmaxLR  0.6529\n",
       "1    RF  0.5743    standardRF  0.5623    minmaxRF  0.5768\n",
       "2   XGB  0.7589   standardXGB  0.7606   minmaxXGB  0.7608\n",
       "3  LGBM  0.7005  standardLGBM  0.6892  minmaxLGBM  0.6942\n",
       "4    GB  0.7481    standardGB  0.7524    minmaxGB  0.7536\n",
       "5   MLP  0.5790   standardMLP  0.5744   minmaxMLP  0.5761"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minmax scaler\n",
    "models = get_scaled_preprocess('minmax')\n",
    "names, results = evaluate(X_train, y_train, models)\n",
    "minmax_scaler_score = create_score_df(names, results)\n",
    "\n",
    "df_score = pd.concat([df_score, minmax_scaler_score], axis=1)\n",
    "df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the result between no scaling, standardscaler and minmaxscaler we can see that minmaxscaler provides the best result. From here on now I'll use minmaxscaler to scale the data.\n",
    "\n",
    "The best performing model by a small margin is the XGBClassifier. This will be our base model and the next step is to see if we can improve the score by tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(clf):\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(mailout_train_cleaned.astype('float'))\n",
    "\n",
    "mailout_train_cleaned_scaled = scaler.transform(mailout_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create minmax scaled train and validation set\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(mailout_train_cleaned_scaled,\n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=SEED,\n",
    "                                                    stratify = target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base XGBClassifier score on validation set: {roc_auc_score(y_test2, preds)}\n"
     ]
    }
   ],
   "source": [
    "xgb_0 = xgb.XGBClassifier(random_state=SEED)\n",
    "xgb_0.fit(X_train2, y_train2)\n",
    "preds = xgb_0.predict_proba(X_test2)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base XGBClassifier score on validation set: 0.7717908188105672\n"
     ]
    }
   ],
   "source": [
    "print(f'Base XGBClassifier score on validation set: {roc_auc_score(y_test2, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer  this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
